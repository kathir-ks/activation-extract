# JIT and Performance Analysis

## Current State of JIT Compilation

### Summary
‚úÖ **Good:** Core model operations are JIT-compiled
‚ö†Ô∏è **Issue:** Activation extraction path is **NOT JIT-compiled** - major performance bottleneck
‚ö†Ô∏è **Issue:** Repeated JIT compilation on every batch due to shape changes
‚ö†Ô∏è **Issue:** Python loops in extraction code prevent full XLA fusion

---

## Detailed Analysis

### 1. Model Implementation (qwen2_jax.py)

**Current State:**
```python
class QwenAttention(nn.Module):
    @nn.compact
    def __call__(self, x):
        # ‚úÖ Automatically JIT-compiled by Flax
        # All attention operations (QKV, matmuls, softmax) are XLA-optimized
```

**Analysis:**
- ‚úÖ **Good:** All Flax `nn.Module` methods are automatically JIT-compiled when used with `model.apply()`
- ‚úÖ **Good:** Attention, MLP, LayerNorm operations fully fused by XLA
- ‚úÖ **Good:** RoPE, KV caching efficiently compiled

**Performance:** Excellent - no issues here

---

### 2. Activation Extraction (extract_activations_fineweb_multihost.py)

#### Issue #1: Main Extraction Function is NOT JIT-compiled

**Current Code (Line 633):**
```python
def extract_activations_sharded(model, params, input_ids):
    """Extract activations with sharded model (JIT compiled) <-- FALSE!"""
    # ‚ùå NO @jax.jit decorator!
    _, _, activations = model.apply(
        params,
        input_ids,
        return_activations=True  # ‚ùå Prevents JIT compilation
    )
    return activations
```

**Problem:**
- The function is **not decorated with @jax.jit**
- Even if it were, `return_activations=True` creates dynamic shapes that break JIT
- Every forward pass re-compiles or runs in eager mode

**Performance Impact:**
- üêå **5-10x slower** than it could be
- üêå Repeated compilation overhead on every batch
- üêå No XLA fusion across layers

#### Issue #2: Processing Loop (extract_activations_arc_v5e64.py, Line 490)

**Current Code:**
```python
for batch_idx in tqdm(range(num_batches), desc="Processing batches"):
    # ‚ùå Python loop - each iteration is separate
    batch_sequences = sequences[start_idx:end_idx]

    # ‚ùå This gets re-compiled if batch shapes change
    process_batch(
        jax_model, params, batch_sequences, ...
    )
```

**Problems:**
1. **Python loop:** Prevents XLA from fusing multiple batches
2. **Variable batch sizes:** Last batch is often smaller ‚Üí triggers recompilation
3. **No batching across sequence dimension:** Could process more in parallel

**Performance Impact:**
- üêå Recompilation on last batch (shapes differ)
- üêå No pipeline parallelism across batches
- üêå Underutilized TPU cores during single-batch processing

#### Issue #3: Activation Storage (process_batch, Line 266)

**Current Code:**
```python
def process_batch(...):
    # Forward pass
    activations = extract_activations_sharded(model, params, input_ids)

    # ‚ùå Python loop - breaks out of JAX context
    for i, sample_idx in enumerate(sample_indices):
        for layer_idx in layers_to_extract:
            # ‚ùå Array conversion in Python loop
            layer_act = activations[layer_key][i]
            layer_act_np = np.array(layer_act)  # ‚ùå Device‚ÜíHost copy

            storage.add_activation(...)  # ‚ùå Python dict operations
```

**Problems:**
1. **Python loops:** Extract activations one at a time
2. **Synchronous device‚Üíhost copies:** Block TPU while copying
3. **Sequential processing:** Can't overlap compute with I/O

**Performance Impact:**
- üêå TPU idle during host copies
- üêå No overlap between computation and storage
- üêå Memory copies dominate runtime for small batches

---

### 3. Generation Code (qwen2_jax_with_hooks.py)

**Current Code (Line 230, 240):**
```python
@jax.jit
def prefill(params, input_ids):
    """‚úÖ JIT-compiled prefill"""
    return model.apply(params, input_ids, ...)

@jax.jit
def decode_step(params, input_id, kv_caches, position):
    """‚úÖ JIT-compiled decode"""
    return model.apply(params, input_id, ...)

# ‚ùå But this one is NOT JIT-compiled:
def decode_step_with_activations(params, input_id, kv_caches, position):
    """‚ùå No @jax.jit when extracting activations"""
    return model.apply(params, input_id, return_activations=True)
```

**Analysis:**
- ‚úÖ **Good:** Generation paths are JIT-compiled
- ‚ùå **Bad:** Activation extraction disables JIT (line 251)

**Impact:**
- Generation: Fast ‚ö°
- Activation extraction: Slow üêå

---

## Performance Bottlenecks Ranked

### Critical (10x+ impact):
1. **No JIT on activation extraction** (extract_activations_sharded)
   - Fix: Use static activation shapes, JIT-compile with donation
   - Expected speedup: **5-10x**

2. **Synchronous device‚Üíhost copies in Python loops**
   - Fix: Use async transfers, batch copies
   - Expected speedup: **2-3x**

### High (2-5x impact):
3. **Variable batch sizes trigger recompilation**
   - Fix: Pad all batches to fixed size, use masking
   - Expected speedup: **2x** (eliminates recompilation)

4. **No pipelining between compute and I/O**
   - Fix: Prefetch next batch while processing current
   - Expected speedup: **1.5-2x**

### Medium (1.2-2x impact):
5. **Python loops in activation extraction**
   - Fix: Vectorize with `jax.vmap`
   - Expected speedup: **1.5x**

6. **Sequential batch processing**
   - Fix: Pipeline multiple batches with `jax.lax.scan`
   - Expected speedup: **1.3x** on multihost

---

## Recommended Improvements

### 1. JIT-Compile Activation Extraction (CRITICAL)

**Current:**
```python
def extract_activations_sharded(model, params, input_ids):
    _, _, activations = model.apply(params, input_ids, return_activations=True)
    return activations
```

**Improved:**
```python
@partial(jax.jit, static_argnums=(0,), donate_argnums=(1,))
def extract_activations_sharded(model, params, input_ids, layer_indices):
    """
    JIT-compiled activation extraction with static layer specification

    Args:
        model: Static argument (model structure)
        params: Donated (will be garbage collected after use if last reference)
        input_ids: [batch, seq_len]
        layer_indices: Static tuple of layer indices to extract
    """
    def forward_with_hooks(params, input_ids):
        # Use static layer indices to avoid dynamic shapes
        _, _, activations = model.apply(
            params, input_ids,
            return_activations=True,
            layer_indices=layer_indices  # Static specification
        )
        return activations

    return forward_with_hooks(params, input_ids)
```

**Benefits:**
- ‚úÖ Full XLA compilation and fusion
- ‚úÖ ~5-10x speedup on activation extraction
- ‚úÖ Reuses compiled code across batches

### 2. Fix Variable Batch Sizes (HIGH PRIORITY)

**Current:**
```python
# Last batch might be smaller ‚Üí triggers recompilation
batch_sequences = sequences[start_idx:end_idx]
```

**Improved:**
```python
def create_fixed_size_batches(sequences, batch_size, pad_token_id):
    """Create all batches with fixed size for JIT stability"""
    batches = []
    masks = []

    for i in range(0, len(sequences), batch_size):
        batch = sequences[i:i + batch_size]

        # Pad batch dimension to fixed size
        if len(batch) < batch_size:
            batch = batch + [sequences[-1]] * (batch_size - len(batch))
            mask = [1] * (i + len(sequences[i:i + batch_size])) + [0] * (batch_size - len(sequences[i:i + batch_size]))
        else:
            mask = [1] * batch_size

        batches.append(batch)
        masks.append(mask)

    return batches, masks

# Use in main loop:
batches, masks = create_fixed_size_batches(sequences, cfg.batch_size, pad_token_id)

for batch, mask in zip(batches, masks):
    # Fixed size ‚Üí single compilation
    activations = extract_activations_sharded(model, params, batch, layer_indices)
    # Use mask to filter out padding
```

**Benefits:**
- ‚úÖ Single compilation for all batches
- ‚úÖ Predictable memory usage
- ‚úÖ Better TPU utilization

### 3. Async Device‚ÜíHost Transfers (HIGH PRIORITY)

**Current:**
```python
for i, sample_idx in enumerate(sample_indices):
    layer_act = activations[layer_key][i]
    layer_act_np = np.array(layer_act)  # ‚ùå Blocking copy
    storage.add_activation(layer_act_np)
```

**Improved:**
```python
# Move all activations to host asynchronously
def transfer_activations_async(activations, layer_indices):
    """Batch transfer all activations to host asynchronously"""
    host_activations = {}

    for layer_idx in layer_indices:
        layer_key = f'layer_{layer_idx}'
        if layer_key in activations:
            # jax.device_get is async - starts transfer, returns immediately
            host_activations[layer_key] = jax.device_get(activations[layer_key])

    return host_activations

# In main loop:
with jax.default_device(jax.devices('cpu')[0]):
    # All transfers happen in parallel, non-blocking
    host_acts = transfer_activations_async(activations, cfg.layers_to_extract)

# Later, when actually needed:
for i, sample_idx in enumerate(sample_indices):
    # Conversion already happened asynchronously
    layer_act_np = np.array(host_acts[layer_key][i])
    storage.add_activation(layer_act_np)
```

**Benefits:**
- ‚úÖ TPU continues computing while transferring previous batch
- ‚úÖ 2-3x speedup by overlapping compute and I/O

### 4. Vectorize Activation Processing (MEDIUM PRIORITY)

**Current:**
```python
# Python loop over samples
for i, sample_idx in enumerate(sample_indices):
    for layer_idx in layers_to_extract:
        layer_act = activations[layer_key][i]
        storage.add_activation(...)
```

**Improved:**
```python
# Vectorized processing
def process_activations_batch(activations, layer_indices):
    """Process entire batch of activations at once"""
    # Stack all layers: [num_layers, batch, seq, hidden]
    stacked = jnp.stack([activations[f'layer_{i}'] for i in layer_indices])

    # Convert to numpy in single operation
    return np.array(stacked)

# In main loop:
acts_np = process_activations_batch(activations, cfg.layers_to_extract)

# Then iterate only over batch dimension (much smaller loop)
for i, sample_idx in enumerate(sample_indices):
    for j, layer_idx in enumerate(cfg.layers_to_extract):
        storage.add_activation(layer_idx, acts_np[j, i], ...)
```

**Benefits:**
- ‚úÖ Single device‚Üíhost transfer instead of many
- ‚úÖ Better memory layout
- ‚úÖ ~1.5x speedup

### 5. Pipeline Multiple Batches (ADVANCED)

**Current:**
```python
for batch in batches:
    activations = extract_activations(batch)  # ‚Üê Wait for completion
    process(activations)                      # ‚Üê Then process
    # Next batch starts only after this one finishes
```

**Improved:**
```python
def pipelined_extraction(batches, model, params):
    """Pipeline computation and I/O"""

    # Start first batch
    future_acts = jax.jit(extract_activations)(model, params, batches[0])

    for i in range(1, len(batches)):
        # While GPU processes batch i, CPU processes batch i-1
        current_acts = future_acts  # Wait for previous
        future_acts = jax.jit(extract_activations)(model, params, batches[i])  # Start next

        # Process current while next is computing
        process_and_store(current_acts)

    # Process last batch
    process_and_store(future_acts)
```

**Benefits:**
- ‚úÖ Overlap computation with I/O
- ‚úÖ ~1.5x throughput improvement

---

## Expected Performance Gains

### Current Performance (estimated):
- **Single batch (32 samples):** ~2-3 seconds
  - Model forward: 0.3s (JIT-compiled ‚úÖ)
  - Activation extraction overhead: 1.5s (not JIT ‚ùå)
  - Device‚Üíhost copy: 0.5s (blocking ‚ùå)
  - Python processing: 0.5s (slow ‚ùå)

### With All Improvements:
- **Single batch (32 samples):** ~0.4-0.5 seconds
  - Model forward: 0.3s (same)
  - Activation extraction: 0.1s (JIT-compiled ‚úÖ, 15x faster)
  - Device‚Üíhost copy: 0.05s (async ‚úÖ, 10x faster)
  - Python processing: 0.05s (vectorized ‚úÖ, 10x faster)

**Overall speedup: 4-7x** üöÄ

### Multihost Specific Benefits:
- **Better:** Load balancing (no stalls waiting for I/O)
- **Better:** Network bandwidth utilization (continuous data flow)
- **Better:** Scalability (near-linear with number of hosts)

---

## Implementation Priority

### Phase 1 (Immediate - 5-10x speedup):
1. ‚úÖ Add `@jax.jit` to `extract_activations_sharded`
2. ‚úÖ Fix variable batch sizes with padding
3. ‚úÖ Use async device‚Üíhost transfers

**Effort:** 2-3 hours
**Gain:** 5-10x speedup

### Phase 2 (Medium-term - additional 1.5-2x):
4. ‚úÖ Vectorize activation processing
5. ‚úÖ Add prefetching/pipelining

**Effort:** 4-6 hours
**Gain:** Additional 1.5-2x speedup

### Phase 3 (Optional - polish):
6. Profile and optimize GCS upload parallelism
7. Add XLA profiling to identify remaining bottlenecks

---

## Monitoring JIT Compilation

### Check if JIT is working:

```python
# Add to code:
import jax

# Enable XLA logging
jax.config.update('jax_log_compiles', True)

# Run extraction
extract_activations_sharded(...)

# You should see:
# "Compiling extract_activations_sharded for [[32, 512]]" (ONCE per shape)
# If you see it repeatedly ‚Üí compilation happening every call (BAD)
```

### Profile with JAX:

```python
with jax.profiler.trace("/tmp/jax-trace", create_perfetto_trace=True):
    for batch in batches[:10]:  # Profile first 10 batches
        extract_activations_sharded(...)

# View at: ui.perfetto.dev
# Look for:
# - Repeated compilations (should be 0 after first batch)
# - Device idle time (should be <5%)
# - Host‚Üídevice copy time (should be overlapped)
```

---

## Conclusion

**Current State:**
- ‚úÖ Model implementation: Excellent JIT usage
- ‚ùå Activation extraction: No JIT, major bottleneck
- ‚ùå I/O handling: Blocking, no pipelining

**Low-hanging fruit (Phase 1):**
- Add `@jax.jit` decorator ‚Üí **5x speedup**
- Fix batch sizes ‚Üí **2x speedup**
- Async transfers ‚Üí **2x speedup**

**Total potential: 10-20x faster with proper JIT usage** üöÄ

The biggest issue is that activation extraction completely bypasses JAX's JIT compilation, running in eager mode. This is easily fixable and will give massive speedup.
