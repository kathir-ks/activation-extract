Starting test inference at Sat Oct  4 18:03:43 UTC 2025
Log file: /home/kathirks_gc/torch_xla/qwen/logs/test_inference_20251004_180343.log
========================================
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
======================================================================
DISTRIBUTED ARC-AGI INFERENCE WITH ACTIVATION EXTRACTION
======================================================================
{
  "output_filepath": "test_outputs/predictions_20251004_180343.json",
  "activations_dir": "test_activations/run_20251004_180343",
  "model_path": "Qwen/Qwen2.5-0.5B",
  "max_model_len": 10240,
  "grid_encoder": "GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))",
  "prompt_version": "output-from-examples-v0",
  "dataset_path": "test_data_small.json",
  "n_tasks": 2,
  "max_output_tokens": 50,
  "predictions_per_task": 2,
  "temperature": 0.0,
  "batch_size": 2,
  "random_seed": null,
  "mesh_shape": [
    2,
    2
  ],
  "use_pjit": true,
  "extract_activations": true,
  "layers_to_extract": [
    10,
    11,
    12
  ],
  "save_every_n_batches": 10,
  "upload_to_cloud": false,
  "cloud_bucket": null,
  "verbose": false
}
======================================================================
Found 4 devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
Created mesh with shape [2, 2]: Mesh('data': 2, 'model': 2, axis_types=(Auto, Auto))
Loaded 2 tasks from test_data_small.json

Loading tokenizer from Qwen/Qwen2.5-0.5B...
Creating model with activation hooks for layers [10, 11, 12]...
Loading model weights from Qwen/Qwen2.5-0.5B...

Replicating model across 4 devices...

Creating prompts for 2 tasks...
Creating prompts:   0%|          | 0/2 [00:00<?, ?it/s]Creating prompts: 100%|██████████| 2/2 [00:00<00:00, 124.16it/s]
Created 3 prompts
Tokenizing prompts...
Tokenizing:   0%|          | 0/3 [00:00<?, ?it/s]Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 85.55it/s]

Distributing data across 4 devices...
Processing 1 batches...
Inference batches:   0%|          | 0/1 [00:00<?, ?it/s]Inference batches:   0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 584, in <module>
    inference_main_distributed()
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 489, in inference_main_distributed
    max_len = max(item['input_ids'].shape[1] for device_batch in batch for item in device_batch)
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 489, in <genexpr>
    max_len = max(item['input_ids'].shape[1] for device_batch in batch for item in device_batch)
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
========================================
Test inference completed at Sat Oct  4 18:04:18 UTC 2025
Exit code: 0
Log saved to: /home/kathirks_gc/torch_xla/qwen/logs/test_inference_20251004_180343.log
