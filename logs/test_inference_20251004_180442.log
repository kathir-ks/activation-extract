Starting test inference at Sat Oct  4 18:04:42 UTC 2025
Log file: /home/kathirks_gc/torch_xla/qwen/logs/test_inference_20251004_180442.log
========================================
WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.
======================================================================
DISTRIBUTED ARC-AGI INFERENCE WITH ACTIVATION EXTRACTION
======================================================================
{
  "output_filepath": "test_outputs/predictions_20251004_180442.json",
  "activations_dir": "test_activations/run_20251004_180442",
  "model_path": "Qwen/Qwen2.5-0.5B",
  "max_model_len": 10240,
  "grid_encoder": "GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))",
  "prompt_version": "output-from-examples-v0",
  "dataset_path": "test_data_small.json",
  "n_tasks": 2,
  "max_output_tokens": 50,
  "predictions_per_task": 2,
  "temperature": 0.0,
  "batch_size": 2,
  "random_seed": null,
  "mesh_shape": [
    2,
    2
  ],
  "use_pjit": true,
  "extract_activations": true,
  "layers_to_extract": [
    10,
    11,
    12
  ],
  "save_every_n_batches": 10,
  "upload_to_cloud": false,
  "cloud_bucket": null,
  "verbose": false
}
======================================================================
Found 4 devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
Created mesh with shape [2, 2]: Mesh('data': 2, 'model': 2, axis_types=(Auto, Auto))
Loaded 2 tasks from test_data_small.json

Loading tokenizer from Qwen/Qwen2.5-0.5B...
Creating model with activation hooks for layers [10, 11, 12]...
Loading model weights from Qwen/Qwen2.5-0.5B...

Replicating model across 4 devices...

Creating prompts for 2 tasks...

Creating prompts:   0%|          | 0/2 [00:00<?, ?it/s]
Creating prompts: 100%|██████████| 2/2 [00:00<00:00, 122.72it/s]
Created 3 prompts
Tokenizing prompts...

Tokenizing:   0%|          | 0/3 [00:00<?, ?it/s]
Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 83.89it/s]

Distributing data across 4 devices...
Processing 1 batches...

Inference batches:   0%|          | 0/1 [00:00<?, ?it/s]
Inference batches:   0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 589, in <module>
    inference_main_distributed()
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 513, in inference_main_distributed
    generated_ids = distributed_generate(
ValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'qwen2_jax_with_hooks.QwenModelWithActivations'>, QwenModelWithActivations(
    # attributes
    config = QwenConfig(vocab_size=151936, hidden_size=896, intermediate_size=4864, num_hidden_layers=24, num_attention_heads=14, num_key_value_heads=2, max_position_embeddings=10240, rope_theta=1000000.0, rms_norm_eps=1e-06, tie_word_embeddings=True, use_sliding_window=False, sliding_window=32768, dtype=<class 'jax.numpy.float32'>)
    layers_to_extract = [10, 11, 12]
). The error was:
Traceback (most recent call last):
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 589, in <module>
  File "/home/kathirks_gc/torch_xla/qwen/distributed_inference_with_activations.py", line 513, in inference_main_distributed
  File "/home/kathirks_gc/.local/lib/python3.10/site-packages/flax/linen/module.py", line 733, in wrapped
TypeError: Failed to hash Flax Module.  The module probably contains unhashable attributes.  Module=QwenModelWithActivations(
    # attributes
    config = QwenConfig(vocab_size=151936, hidden_size=896, intermediate_size=4864, num_hidden_layers=24, num_attention_heads=14, num_key_value_heads=2, max_position_embeddings=10240, rope_theta=1000000.0, rms_norm_eps=1e-06, tie_word_embeddings=True, use_sliding_window=False, sliding_window=32768, dtype=<class 'jax.numpy.float32'>)
    layers_to_extract = [10, 11, 12]
)

========================================
Test inference completed at Sat Oct  4 18:05:17 UTC 2025
Exit code: 0
Log saved to: /home/kathirks_gc/torch_xla/qwen/logs/test_inference_20251004_180442.log
