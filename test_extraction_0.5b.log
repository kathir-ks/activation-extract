/usr/lib/python3/dist-packages/pytz/__init__.py:31: SyntaxWarning: invalid escape sequence '\s'
  match = re.match("^#\s*version\s*([0-9a-z]*)\s*$", line)
======================================================================
FINEWEB-EDU ACTIVATION EXTRACTION - MACHINE 0/0
======================================================================
{
  "machine_id": 0,
  "total_machines": 1,
  "model_path": "Qwen/Qwen2.5-0.5B",
  "dataset_name": "HuggingFaceFW/fineweb-edu",
  "dataset_config": "sample-10BT",
  "dataset_split": "train",
  "max_samples": 10,
  "layers_to_extract": [
    20,
    21,
    22,
    23
  ],
  "batch_size": 4,
  "max_seq_length": 256,
  "output_dir": "./test_activations_0.5b",
  "upload_to_gcs": false,
  "gcs_bucket": null,
  "gcs_prefix": "activations_fineweb",
  "shard_size_gb": 0.05,
  "compress_shards": true,
  "delete_local_after_upload": false,
  "verbose": true,
  "use_data_parallel": true
}
======================================================================

Machine 0 - Found 4 device(s): ['TPU v4', 'TPU v4', 'TPU v4', 'TPU v4']

Loading dataset shard for machine 0/0...
  Dataset: HuggingFaceFW/fineweb-edu
  Config: sample-10BT
  Split: train
  ✓ Dataset shard 0 loaded (streaming mode)

Detecting model configuration from Qwen/Qwen2.5-0.5B...
  ✓ Loaded config from HuggingFace
  Model: qwen2
  Hidden size: 896
  Layers: 24
  Attention heads: 14
  Vocab size: 151936
Loading tokenizer from Qwen/Qwen2.5-0.5B...
Loading HF model for weight conversion...
Creating JAX model with activation hooks for layers [20, 21, 22, 23]...
Converting HF weights to JAX...

Setting up model sharding across 4 devices...
  ✓ Created mesh with axis: ('model',)
  ✓ Created sharding strategy with 12 rules
  ⟳ Sharding parameters across devices...
Traceback (most recent call last):
  File "/home/kathirks_gc/.local/lib/python3.12/site-packages/jax/_src/pjit.py", line 1437, in pjit_check_aval_sharding
    s.check_compatible_aval(shape)
  File "/home/kathirks_gc/.local/lib/python3.12/site-packages/jax/_src/named_sharding.py", line 173, in check_compatible_aval
    raise ValueError(
ValueError: Sharding NamedSharding(mesh=Mesh('model': 4, axis_types=(Auto,)), spec=PartitionSpec(None, 'model'), memory_kind=device) is only valid for values of rank at least 2, but was applied to a value of rank 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 852, in <module>
    main()
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 798, in main
    params = shard_params(params, mesh, sharding_rules)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 456, in shard_params
    return _shard_tree(params)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 448, in _shard_tree
    k: _shard_tree(v, path + (k,))
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 448, in _shard_tree
    k: _shard_tree(v, path + (k,))
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 448, in _shard_tree
    k: _shard_tree(v, path + (k,))
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 2 more times]
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 452, in _shard_tree
    return shard_array(path, jnp.array(params_subtree))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kathirks_gc/torch_xla/qwen/extract_activations_fineweb.py", line 442, in shard_array
    return jax.device_put(value, sharding)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kathirks_gc/.local/lib/python3.12/site-packages/jax/_src/api.py", line 2727, in device_put
    _check_sharding(aval, d)
  File "/home/kathirks_gc/.local/lib/python3.12/site-packages/jax/_src/api.py", line 2629, in _check_sharding
    pjit.pjit_check_aval_sharding(
  File "/home/kathirks_gc/.local/lib/python3.12/site-packages/jax/_src/pjit.py", line 1441, in pjit_check_aval_sharding
    raise ValueError(
ValueError: One of device_put args is incompatible with its sharding annotation NamedSharding(mesh=Mesh('model': 4, axis_types=(Auto,)), spec=PartitionSpec(None, 'model'), memory_kind=device): Sharding NamedSharding(mesh=Mesh('model': 4, axis_types=(Auto,)), spec=PartitionSpec(None, 'model'), memory_kind=device) is only valid for values of rank at least 2, but was applied to a value of rank 1.
