/usr/lib/python3/dist-packages/pytz/__init__.py:31: SyntaxWarning: invalid escape sequence '\s'
  match = re.match("^#\s*version\s*([0-9a-z]*)\s*$", line)
======================================================================
FINEWEB-EDU ACTIVATION EXTRACTION - MACHINE 0/0
======================================================================
{
  "machine_id": 0,
  "total_machines": 1,
  "model_path": "Qwen/Qwen2.5-0.5B",
  "dataset_name": "HuggingFaceFW/fineweb-edu",
  "dataset_config": "sample-10BT",
  "dataset_split": "train",
  "max_samples": 10,
  "layers_to_extract": [
    20,
    21,
    22,
    23
  ],
  "batch_size": 4,
  "max_seq_length": 256,
  "output_dir": "./test_activations_0.5b",
  "upload_to_gcs": false,
  "gcs_bucket": null,
  "gcs_prefix": "activations_fineweb",
  "shard_size_gb": 0.05,
  "compress_shards": true,
  "delete_local_after_upload": false,
  "verbose": true,
  "use_data_parallel": true
}
======================================================================

Machine 0 - Found 4 device(s): ['TPU v4', 'TPU v4', 'TPU v4', 'TPU v4']

Loading dataset shard for machine 0/0...
  Dataset: HuggingFaceFW/fineweb-edu
  Config: sample-10BT
  Split: train
  ✓ Dataset shard 0 loaded (streaming mode)

Detecting model configuration from Qwen/Qwen2.5-0.5B...
  ✓ Loaded config from HuggingFace
  Model: qwen2
  Hidden size: 896
  Layers: 24
  Attention heads: 14
  Vocab size: 151936
Loading tokenizer from Qwen/Qwen2.5-0.5B...
Loading HF model for weight conversion...
Creating JAX model with activation hooks for layers [20, 21, 22, 23]...
Converting HF weights to JAX...

Setting up model sharding across 4 devices...
  ✓ Created mesh with axis: ('model',)
  ✓ Created sharding strategy with 12 rules
  ⟳ Sharding parameters across devices...
  ✓ Parameters sharded successfully

  Memory distribution across devices:
    Device 0 (TPU v4): Ready
    Device 1 (TPU v4): Ready
    Device 2 (TPU v4): Ready
    Device 3 (TPU v4): Ready

Extracting activations from layers [20, 21, 22, 23]...
Mode: Model Sharding
Extracting activations (sharded): 0it [00:00, ?it/s]Extracting activations (sharded): 1it [00:01,  1.73s/it]Extracting activations (sharded): 5it [00:22,  4.65s/it]Extracting activations (sharded): 10it [00:42,  4.29s/it]

======================================================================
Saving shard 1: shard_0001.pkl.gz (~35.0 MB)
======================================================================
  ✓ Saved locally: 32.5 MB
    Layer 20: 10 samples
    Layer 21: 10 samples
    Layer 22: 10 samples
    Layer 23: 10 samples

======================================================================
STORAGE SUMMARY
======================================================================
  Total shards: 1
  Total samples: 40
  Metadata: test_activations_0.5b/metadata.json
======================================================================

======================================================================
MACHINE 0 - EXTRACTION COMPLETE!
Activations saved to: ./test_activations_0.5b
======================================================================
